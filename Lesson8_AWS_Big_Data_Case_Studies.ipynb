{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson8-AWS-Big-Data-Case-Studies.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "ZAwxrTUcfeko",
        "OVuZQBAg-gHf",
        "nl1K5TVY00Pg",
        "iKGPvalK02Yt",
        "F9YxzTDT04lq",
        "1Q52AI5CW3eh",
        "yc--CN_Kolxo",
        "MD7T7q0TmHWZ",
        "_tAhECAQfCPB",
        "QgMjuk6kxi2u",
        "rKXgIaJzsTzC",
        "INrZ6YMcwFxp",
        "awt-CAP5wNOF",
        "6fl3b_w6wJJM",
        "jyZaUa-NOWYr",
        "DHqZDYOmsj8G"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noahgift/1minutetip/blob/master/Lesson8_AWS_Big_Data_Case_Studies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Wy6nhdcaVsCH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 8 Case Studies"
      ]
    },
    {
      "metadata": {
        "id": "c_Id55m6Jsbu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pragmatic AI Labs\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "e5p96AqpSDZa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://paiml.com/images/logo_with_slogan_white_background.png)\n",
        "\n",
        "This notebook was produced by [Pragmatic AI Labs](https://paiml.com/).  You can continue learning about these topics by:\n",
        "\n",
        "*   Buying a copy of [Pragmatic AI: An Introduction to Cloud-Based Machine Learning](http://www.informit.com/store/pragmatic-ai-an-introduction-to-cloud-based-machine-9780134863917)\n",
        "*   Reading an online copy of [Pragmatic AI:Pragmatic AI: An Introduction to Cloud-Based Machine Learning](https://www.safaribooksonline.com/library/view/pragmatic-ai-an/9780134863924/)\n",
        "*  Watching video [Essential Machine Learning and AI with Python and Jupyter Notebook-Video-SafariOnline](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118) on Safari Books Online.\n",
        "* Watching video [AWS Certified Machine Learning-Speciality](https://learning.oreilly.com/videos/aws-certified-machine/9780135556597)\n",
        "* Purchasing video [Essential Machine Learning and AI with Python and Jupyter Notebook- Purchase Video](http://www.informit.com/store/essential-machine-learning-and-ai-with-python-and-jupyter-9780135261095)\n",
        "* Purchasing video [AWS Certified Machine Learning Video and Practice Exams](http://www.pearsonitcertification.com/store/aws-certified-machine-learning-specialty-ml-s-complete-9780135556511)\n",
        "*   Viewing more content at [noahgift.com](https://noahgift.com/)\n"
      ]
    },
    {
      "metadata": {
        "id": "275I4g3bvxaP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lesson 8:  Case Studies (60 min)\n",
        "\n",
        "* 8.1 Understand Big Data for Sagemaker (12 min)\n",
        "* 8.2 Learn Sagemaker and EMR Integration (12 min)\n",
        "* 8.3 Learn Serverless Production Big Data Application Development (12 min)\n",
        "* 8.4 Implement Containerization for Big Data (12 min)\n",
        "* 8.5 Implement Spot Instances for Big Data Pipeline (12 min)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "X8kZHKqie1RM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 8.1 Understand Big Data for Sagemaker"
      ]
    },
    {
      "metadata": {
        "id": "ZAwxrTUcfeko",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Search"
      ]
    },
    {
      "metadata": {
        "id": "xJN-eXYDtpVE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### [Demo] Search"
      ]
    },
    {
      "metadata": {
        "id": "60kLF_hRfgZd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Manage Machine Learning Experiments with Search](https://docs.aws.amazon.com/sagemaker/latest/dg/search.html)\n",
        "\n",
        "\n",
        "\n",
        "*   Finding training jobs\n",
        "*   Rank training jobs\n",
        "*   Tracing lineage of a model\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "OVuZQBAg-gHf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Ground Truth"
      ]
    },
    {
      "metadata": {
        "id": "MYIx7nJT0z2z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![ground_truth](https://user-images.githubusercontent.com/58792/49688683-9bdba100-faca-11e8-8d93-a55ce6c35a92.png)\n",
        "\n",
        "\n",
        "\n",
        "*   Setup and Manage labeling jobs\n",
        "*   Uses active learning and human labeling\n",
        "*   First 500 objects labeled per month are free\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xfvNv4E237k9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### [Demo] Labeling Job"
      ]
    },
    {
      "metadata": {
        "id": "0pU-_czP4Blj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "nl1K5TVY00Pg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Notebook"
      ]
    },
    {
      "metadata": {
        "id": "guMyo-9P014a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![notebooks](https://user-images.githubusercontent.com/58792/49688694-d04f5d00-faca-11e8-9fad-eb63b2534b07.png)"
      ]
    },
    {
      "metadata": {
        "id": "o6nEwQ1tGiOl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### [Demo] Sagemaker Notebooks\n",
        "\n",
        "*   Create and run Jupyter Notebooks\n",
        "  -  Using Jupyter\n",
        "  -  Using JupyterLab\n",
        "  -  Using the terminal\n",
        "  \n",
        "*   Lifecycle configurations\n",
        "\n",
        "*   Git Repositories\n",
        "  - public repositories can be cloned on Notebook launch\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "iKGPvalK02Yt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "WwNsFi6602eK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![training](https://user-images.githubusercontent.com/58792/49688717-05f44600-facb-11e8-8d7f-cf33d272573a.png)"
      ]
    },
    {
      "metadata": {
        "id": "UGQQyDJtUN6D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### [Demo] Sagemaker Training\n",
        "\n",
        "*   Algorithms\n",
        "  -  Create algorithm\n",
        "  -  Subscribe [AWS Marketplace](https://aws.amazon.com/marketplace/search/results?page=1&filters=fulfillment_options%2Cresource_type&fulfillment_options=SAGEMAKER&resource_type=ALGORITHM)\n",
        "\n",
        "  \n",
        "*   Training Jobs\n",
        "\n",
        "*   HyperParameter Tuning Jobs\n"
      ]
    },
    {
      "metadata": {
        "id": "F9YxzTDT04lq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ]
    },
    {
      "metadata": {
        "id": "9OALWADP05aM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![inference](https://user-images.githubusercontent.com/58792/49688735-2fad6d00-facb-11e8-94cb-cba9322e309b.png)"
      ]
    },
    {
      "metadata": {
        "id": "RQqvKpzIp1Sl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### [Demo] Sagemaker Inference\n",
        "\n",
        "*  Compilation jobs\n",
        "\n",
        "*  Model packages\n",
        "\n",
        "*  Models\n",
        "\n",
        "*  Endpoint configurations\n",
        "\n",
        "*  Endpoints\n",
        "\n",
        "*  Batch transform jobs\n"
      ]
    },
    {
      "metadata": {
        "id": "1Q52AI5CW3eh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Built in Sagemaker Algorithms"
      ]
    },
    {
      "metadata": {
        "id": "NzsQUYX7W5Xa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Table of [algorithms provided by Amazon Sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)\n",
        "\n",
        "![aws_algorithms](https://user-images.githubusercontent.com/58792/49692597-58595500-fb13-11e8-9db3-e1fe371ac36a.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "E4mpkoTycugo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yc--CN_Kolxo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sagemaker Built-in Algorithms---Examples\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QOi-EXNUoz4O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### BlazingText\n",
        "\n",
        "\n",
        "* unsupervised learning algorithm for generating **Word2Vec embeddings.**\n",
        "* aws blog post [BlazingText](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-blazingtext-parallelizing-word2vec-on-multiple-cpus-or-gpus/)\n",
        "\n",
        "\n",
        "\n",
        "![BlazingText](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2018/01/18/sagemaker-word2vec-3-1.gif)"
      ]
    },
    {
      "metadata": {
        "id": "zCtotY8NpxfW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### DeepAR Forecasting\n",
        "\n",
        "* supervised learning algorithm for forecasting scalar (that is, one-dimensional) time series using recurrent neural networks (RNN)\n",
        "* [DeepAR Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html)\n",
        "\n",
        "![DeepAR](https://docs.aws.amazon.com/sagemaker/latest/dg/images/deepar-2.png)"
      ]
    },
    {
      "metadata": {
        "id": "MD7T7q0TmHWZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Demo\n"
      ]
    },
    {
      "metadata": {
        "id": "zUQJRosYe7Q6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Demo]\n",
        "\n",
        "* **Built in Sagemaker Algorithms Scale:   **\n",
        "\n",
        "---\n",
        "\n",
        "\"We recommend training k-means on CPU instances. You can train on GPU instances, but should limit GPU training to p*.xlarge instances because only one GPU \n",
        "per instance is used.\"\"\n",
        "\n",
        "\n",
        "* County Census Notebook\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_tAhECAQfCPB"
      },
      "cell_type": "markdown",
      "source": [
        "## 8.2 Learn Sagemaker and EMR Integration"
      ]
    },
    {
      "metadata": {
        "id": "6zFHAEo1qeog",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![kernel](https://user-images.githubusercontent.com/58792/55046667-982f4400-4fff-11e9-8db1-b70766de3f52.png)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Y-rwz3uZfCPD"
      },
      "cell_type": "markdown",
      "source": [
        "### Demo\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1252IBFEmghS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![EMR](https://user-images.githubusercontent.com/58792/55369064-d1f1c600-54a9-11e9-9937-9ded09489c6d.png)\n",
        "\n",
        "* [Sagemaker/Spark/EMR Notebooks](https://aws.amazon.com/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/)"
      ]
    },
    {
      "metadata": {
        "id": "QgMjuk6kxi2u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 8.3 Learn Serverless Production Big Data Application Development"
      ]
    },
    {
      "metadata": {
        "id": "A0NvCIcChKca",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Source Code for Demo](https://github.com/noahgift/awslambda/tree/master/example_src)\n",
        "\n",
        "[Demo]"
      ]
    },
    {
      "metadata": {
        "id": "rKXgIaJzsTzC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating Timed Lambdas"
      ]
    },
    {
      "metadata": {
        "id": "CQ45bnlfN14W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating Serverless Data Pipeline Producers"
      ]
    },
    {
      "metadata": {
        "id": "INrZ6YMcwFxp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Using AWS Lambda with Cloudwatch Events"
      ]
    },
    {
      "metadata": {
        "id": "Fo7rh-wkwJGO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Can create [cloudwatch timer](https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html) to call lambda\n",
        "\n",
        "![cloudwatch event lambda](https://user-images.githubusercontent.com/58792/53612460-4c67b700-3b87-11e9-8fb9-b5d30b77431a.png)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "awt-CAP5wNOF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Using AWS Cloudwatch logging with AWS Lambda"
      ]
    },
    {
      "metadata": {
        "id": "AYCfPjkGwNRB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using cloudwatch logging is an essential step for Lambda Development\n",
        "\n",
        "![cloudwatch](https://user-images.githubusercontent.com/58792/53612528-9355ac80-3b87-11e9-8473-ab28ba860553.png)"
      ]
    },
    {
      "metadata": {
        "id": "6fl3b_w6wJJM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Using AWS Lambda to populate AWS SQS (Simple Queuing Service)"
      ]
    },
    {
      "metadata": {
        "id": "tOqVcZTbwLZg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. *** Create new Lambda with Serverless Wizard***\n",
        "2.  ***cd into lambda and install packages on level up***\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "pip3 install boto3 --target ../\n",
        "pip3 install python-json-logger --target ../\n",
        "```\n",
        "\n",
        "3.  ***Test local***\n",
        "4. *** Deploy***"
      ]
    },
    {
      "metadata": {
        "id": "4Xe5mNrQIQcJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "Dynamo to SQS\n",
        "\"\"\"\n",
        "\n",
        "import boto3\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "\n",
        "DYNAMODB = boto3.resource('dynamodb')\n",
        "TABLE = \"fang\"\n",
        "QUEUE = \"producer\"\n",
        "SQS = boto3.client(\"sqs\")\n",
        "\n",
        "#SETUP LOGGING\n",
        "import logging\n",
        "from pythonjsonlogger import jsonlogger\n",
        "\n",
        "LOG = logging.getLogger()\n",
        "LOG.setLevel(logging.INFO)\n",
        "logHandler = logging.StreamHandler()\n",
        "formatter = jsonlogger.JsonFormatter()\n",
        "logHandler.setFormatter(formatter)\n",
        "LOG.addHandler(logHandler)\n",
        "\n",
        "def scan_table(table):\n",
        "    \"\"\"Scans table and return results\"\"\"\n",
        "    \n",
        "    LOG.info(f\"Scanning Table {table}\")\n",
        "    producer_table = DYNAMODB.Table(table)\n",
        "    response = producer_table.scan()\n",
        "    items = response['Items']\n",
        "    LOG.info(f\"Found {len(items)} Items\")\n",
        "    return items\n",
        "\n",
        "def send_sqs_msg(msg, queue_name, delay=0):\n",
        "    \"\"\"Send SQS Message\n",
        "\n",
        "    Expects an SQS queue_name and msg in a dictionary format.\n",
        "    Returns a response dictionary. \n",
        "    \"\"\"\n",
        "\n",
        "    queue_url = SQS.get_queue_url(QueueName=queue_name)[\"QueueUrl\"]\n",
        "    queue_send_log_msg = \"Send message to queue url: %s, with body: %s\" %\\\n",
        "        (queue_url, msg)\n",
        "    LOG.info(queue_send_log_msg)\n",
        "    json_msg = json.dumps(msg)\n",
        "    response = SQS.send_message(\n",
        "        QueueUrl=queue_url,\n",
        "        MessageBody=json_msg,\n",
        "        DelaySeconds=delay)\n",
        "    queue_send_log_msg_resp = \"Message Response: %s for queue url: %s\" %\\\n",
        "        (response, queue_url) \n",
        "    LOG.info(queue_send_log_msg_resp)\n",
        "    return response\n",
        "\n",
        "def send_emissions(table, queue_name):\n",
        "    \"\"\"Send Emissions\"\"\"\n",
        "    \n",
        "    items = scan_table(table=table)\n",
        "    for item in items:\n",
        "        LOG.info(f\"Sending item {item} to queue: {queue_name}\")\n",
        "        response = send_sqs_msg(item, queue_name=queue_name)\n",
        "        LOG.debug(response)\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    \"\"\"\n",
        "    Lambda entrypoint\n",
        "    \"\"\"\n",
        "\n",
        "    extra_logging = {\"table\": TABLE, \"queue\": QUEUE}\n",
        "    LOG.info(f\"event {event}, context {context}\", extra=extra_logging)\n",
        "    send_emissions(table=TABLE, queue_name=QUEUE)\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PdB-FUHUJ84S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Successful Local Test**\n",
        "\n",
        "![test local](https://user-images.githubusercontent.com/58792/53637263-8bbdf400-3bd7-11e9-9840-0cb9851fac6a.png)\n",
        "\n",
        "**Verify Messages in SQS**\n",
        "\n",
        "![**SQS**](https://user-images.githubusercontent.com/58792/53637424-fb33e380-3bd7-11e9-8b68-021704da4ce0.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "vAStK9C0NDd4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***Remote Test Needs Correct Role!!!***\n",
        "\n",
        "![role failure](https://user-images.githubusercontent.com/58792/53638025-c45ecd00-3bd9-11e9-848c-6caedc3d9011.png)"
      ]
    },
    {
      "metadata": {
        "id": "jyZaUa-NOWYr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Wire up Cloudwatch Event Trigger"
      ]
    },
    {
      "metadata": {
        "id": "Jf353R-xOcBG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1.  Enable Timed Execution of producer\n",
        "2.  Verify messages flowing into SQS\n",
        "\n",
        "![cloudwatch event trigger](https://user-images.githubusercontent.com/58792/53638200-6979a580-3bda-11e9-94ea-9008bdc9c72a.png)\n",
        "\n",
        "***SQS is populating***\n",
        "\n",
        "![alt text](https://user-images.githubusercontent.com/58792/53638351-cecd9680-3bda-11e9-85bb-f5f4bd4450ad.png)"
      ]
    },
    {
      "metadata": {
        "id": "_iIu63uesj5R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating Event Driven Lambdas"
      ]
    },
    {
      "metadata": {
        "id": "DHqZDYOmsj8G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Triggering AWS Lambda with AWS SQS Events"
      ]
    },
    {
      "metadata": {
        "id": "nohx7lBy2khs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lambda can now fire on SQS event\n",
        "\n",
        "![SQS Trigger](https://user-images.githubusercontent.com/58792/53644659-f842ee00-3beb-11e9-8527-96ec12acc5f7.png)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "EzwngIdr2kk2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Reading AWS SQS Events from AWS Lambda"
      ]
    },
    {
      "metadata": {
        "id": "8hvf6Vc62ns3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "def lambda_handler(event, context):\n",
        "    \"\"\"Entry Point for Lambda\"\"\"\n",
        "\n",
        "    LOG.info(f\"SURVEYJOB LAMBDA, event {event}, context {context}\")\n",
        "    receipt_handle  = event['Records'][0]['receiptHandle'] #sqs message\n",
        "    #'eventSourceARN': 'arn:aws:sqs:us-east-1:561744971673:producer'\n",
        "    event_source_arn = event['Records'][0]['eventSourceARN']\n",
        "    \n",
        "    names = [] #Captured from Queue\n",
        "    \n",
        "    # Process Queue\n",
        "    for record in event['Records']:\n",
        "        body = json.loads(record['body'])\n",
        "        company_name = body['name']\n",
        "        \n",
        "        #Capture for processing\n",
        "        names.append(company_name)\n",
        "        \n",
        "        extra_logging = {\"body\": body, \"company_name\":company_name}\n",
        "        LOG.info(f\"SQS CONSUMER LAMBDA, splitting sqs arn with value: {event_source_arn}\",extra=extra_logging)\n",
        "        qname = event_source_arn.split(\":\")[-1]\n",
        "        extra_logging[\"queue\"] = qname\n",
        "        LOG.info(f\"Attemping Deleting SQS receiptHandle {receipt_handle} with queue_name {qname}\", extra=extra_logging)\n",
        "        res = delete_sqs_msg(queue_name=qname, receipt_handle=receipt_handle)\n",
        "        LOG.info(f\"Deleted SQS receipt_handle {receipt_handle} with res {res}\", extra=extra_logging)\n",
        "    \n",
        "    # Make Pandas dataframe with wikipedia snippts\n",
        "    LOG.info(f\"Creating dataframe with values: {names}\")\n",
        "    df = names_to_wikipedia(names)\n",
        "    \n",
        "    # Perform Sentiment Analysis\n",
        "    df = apply_sentiment(df)\n",
        "    LOG.info(f\"Sentiment from FANG companies: {df.to_dict()}\")\n",
        "    \n",
        "    # Write result to S3\n",
        "    write_s3(df=df, name=names.pop(), bucket=\"fangsentiment\")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ckRibEoh2n0q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Writing results to AWS S3"
      ]
    },
    {
      "metadata": {
        "id": "JDn4JdWsqrwy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "write dataframe to AWS S3\n",
        "\n",
        "```python\n",
        "### S3\n",
        "def write_s3(df, name, bucket):\n",
        "    \"\"\"Write S3 Bucket\"\"\"\n",
        "\n",
        "    csv_buffer = StringIO()\n",
        "    df.to_csv(csv_buffer)\n",
        "    s3_resource = boto3.resource('s3')\n",
        "    res = s3_resource.Object(bucket, f'{name}_sentiment.csv').\\\n",
        "        put(Body=csv_buffer.getvalue())\n",
        "    LOG.info(f\"result of write name: {name} to bucket: {bucket} with:\\n {res}\")\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "dpIE4sgc2tZi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "noah:/tmp $ aws s3 cp --recursive s3://fangsentiment/ .                                                                                                \n",
        "download: s3://fangsentiment/netflix_sentiment.csv to ./netflix_sentiment.csv\n",
        "download: s3://fangsentiment/google_sentiment.csv to ./google_sentiment.csv\n",
        "download: s3://fangsentiment/facebook_sentiment.csv to ./facebook_sentiment.csv\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GmxGK04TgIef",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 8.4 Implement Containerization for Big Data"
      ]
    },
    {
      "metadata": {
        "id": "6ShsOjCQgpTH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Demo]"
      ]
    },
    {
      "metadata": {
        "id": "UkfCS4adhVlA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 8.5 Implement Spot Instances for Big Data Pipeline"
      ]
    },
    {
      "metadata": {
        "id": "PG5mslHioxxM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Real Massively Parallel Computer Vision Pipeline**\n",
        "\n",
        "![Spot Pipeline](https://user-images.githubusercontent.com/58792/55369313-ed110580-54aa-11e9-83d5-724611b6f8bd.png)"
      ]
    },
    {
      "metadata": {
        "id": "m4qZ3_nssfGk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Spot Launcher](https://github.com/noahgift/spot_price_machine_learning/blob/master/spot_launcher.py)\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "#!/usr/bin/env python\n",
        "\"\"\"Launches a test spot instance\"\"\"\n",
        "\n",
        "import click\n",
        "import boto3\n",
        "import base64\n",
        "\n",
        "from sensible.loginit import logger\n",
        "log = logger(__name__)\n",
        "\n",
        "#Tell Boto3 To Enable Debug Logging\n",
        "#boto3.set_stream_logger(name='botocore')\n",
        "\n",
        "@click.group()\n",
        "def cli():\n",
        "    \"\"\"Spot Launcher\"\"\"\n",
        "\n",
        "\n",
        "def user_data_cmds(duration):\n",
        "    \"\"\"Initial cmds to run, takes duration for halt cmd\"\"\"\n",
        "\n",
        "    cmds = \"\"\"\n",
        "        #cloud-config\n",
        "        runcmd:\n",
        "         - echo \"halt\" | at now + {duration} min\n",
        "    \"\"\".format(duration=duration)\n",
        "    return cmds\n",
        "\n",
        "@cli.command(\"launch\")\n",
        "@click.option('--instance', default=\"r4.large\", help='Instance Type')\n",
        "@click.option('--duration', default=\"55\", help='Duration')\n",
        "@click.option('--keyname', default=\"pragai\", help='Key Name')\n",
        "@click.option('--profile', default=\"arn:aws:iam::561744971673:instance-profile/admin\",\n",
        "                     help='IamInstanceProfile')\n",
        "@click.option('--securitygroup', default=\"sg-61706e07\", help='Key Name')\n",
        "@click.option('--ami', default=\"ami-6df1e514\", help='Key Name')\n",
        "def request_spot_instance(duration, instance, keyname, \n",
        "                            profile, securitygroup, ami):\n",
        "    \"\"\"Request spot instance\"\"\"\n",
        "\n",
        "    #import pdb;pdb.set_trace()\n",
        "    user_data = user_data_cmds(duration)\n",
        "    LaunchSpecifications = {\n",
        "            \"ImageId\": ami,\n",
        "            \"InstanceType\": instance,\n",
        "            \"KeyName\": keyname,\n",
        "            \"IamInstanceProfile\": {\n",
        "                \"Arn\": profile\n",
        "            },\n",
        "            \"UserData\": base64.b64encode(user_data.encode(\"ascii\")).\\\n",
        "                decode('ascii'),\n",
        "            \"BlockDeviceMappings\": [\n",
        "                {\n",
        "                    \"DeviceName\": \"/dev/xvda\",\n",
        "                    \"Ebs\": {\n",
        "                        \"DeleteOnTermination\": True,\n",
        "                        \"VolumeType\": \"gp2\",\n",
        "                        \"VolumeSize\": 8,\n",
        "                    }\n",
        "                }\n",
        "            ],\n",
        "            \"SecurityGroupIds\": [securitygroup]\n",
        "        }\n",
        "\n",
        "    run_args = {\n",
        "            'SpotPrice'           : \"0.8\",\n",
        "            'Type'                : \"one-time\",\n",
        "            'InstanceCount'       : 1,\n",
        "            'LaunchSpecification' : LaunchSpecifications\n",
        "        }\n",
        "\n",
        "    msg_user_data = \"SPOT REQUEST DATA: %s\" % run_args\n",
        "    log.info(msg_user_data)\n",
        "\n",
        "    client = boto3.client('ec2', \"us-west-2\")\n",
        "    reservation = client.request_spot_instances(**run_args)\n",
        "    return reservation\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    cli()\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zzyil6lfnG0d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Demo"
      ]
    },
    {
      "metadata": {
        "id": "TkQMz_c0hVsU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "* Spot Launch Demo and Walkthrough on Pricing\n",
        "* Spot Instances EMR\n",
        "* Spot Instances AWS Batch"
      ]
    },
    {
      "metadata": {
        "id": "wvTTQ7vAlZL6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}